import streamlit as st
import logging
import requests
from PIL import Image
import torch
from transformers import (
    BlipProcessor,
    BlipForConditionalGeneration,
    VisionEncoderDecoderModel,
    ViTImageProcessor,
    AutoTokenizer,
    T5Tokenizer,
    T5ForConditionalGeneration,
)

# Suppress excessive logging from transformers
logging.getLogger("transformers").setLevel(logging.ERROR)

##############################
# Caption Generation Functions
##############################

# BLIP Base Model
def load_blip_model():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base", use_fast=True)
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

def generate_caption_blip(processor, model, image):
    inputs = processor(image, return_tensors="pt")
    output = model.generate(**inputs)
    caption = processor.decode(output[0], skip_special_tokens=True)
    return caption

# ViT-GPT2 Model
def load_vit_gpt2_model():
    model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning", use_fast=True)
    tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    return model, feature_extractor, tokenizer, device

gen_kwargs = {"max_length": 16, "num_beams": 4}

def generate_caption_vit(image, model, feature_extractor, tokenizer, device):
    if image.mode != "RGB":
        image = image.convert("RGB")
    pixel_values = feature_extractor(images=[image], return_tensors="pt").pixel_values.to(device)
    output_ids = model.generate(pixel_values, **gen_kwargs)
    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    return preds[0].strip()

# BLIP Large Model
def load_blip_large_model():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large", use_fast=True)
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")
    return processor, model

def generate_caption_blip_large(processor, model, image):
    inputs = processor(image, return_tensors="pt")
    output = model.generate(**inputs)
    caption = processor.decode(output[0], skip_special_tokens=True)
    return caption

##############################
# T5-Based Social Media Caption Generation Function
##############################

t5_tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")
t5_model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large")

def generate_social_media_captions(captions, num_outputs=3):
    """
    Given a list of caption strings, generate multiple creative social media captions 
    using similar words and include relevant hashtags.
    Returns a list of caption strings.
    """
    combined_text = " ".join(captions)
    prompt = f"Rewrite the following text as creative social media captions with similar words and include relevant hashtags: {combined_text}"
    input_ids = t5_tokenizer(prompt, return_tensors="pt").input_ids
    outputs = t5_model.generate(
        input_ids,
        max_new_tokens=100,
        num_beams=5,
        num_return_sequences=num_outputs,
        early_stopping=True
    )
    social_captions = [t5_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
    return social_captions

##############################
# Streamlit App
##############################

st.title("Multi-Model Image Captioning & Social Media Caption Generator")
st.write("Upload an image and get captions generated by three models plus creative social media captions!")

uploaded_file = st.file_uploader("Choose an image file", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="Uploaded Image", use_column_width=True)
    
    # Generate captions using the three models
    with st.spinner("Generating BLIP Base caption..."):
        proc_base, model_blip = load_blip_model()
        caption_blip = generate_caption_blip(proc_base, model_blip, image)
    
    with st.spinner("Generating ViT-GPT2 caption..."):
        model_vit, feat_extractor_vit, tokenizer_vit, device = load_vit_gpt2_model()
        caption_vit = generate_caption_vit(image, model_vit, feat_extractor_vit, tokenizer_vit, device)
    
    with st.spinner("Generating BLIP Large caption..."):
        proc_large, model_blip_large = load_blip_large_model()
        caption_blip_large = generate_caption_blip_large(proc_large, model_blip_large, image)
    
    # Generate multiple social media captions using T5
    with st.spinner("Generating creative social media captions..."):
        social_captions = generate_social_media_captions([caption_blip, caption_vit, caption_blip_large], num_outputs=3)
    
    # Combine all results into one array
    results = [caption_blip, caption_vit, caption_blip_large, social_captions]
    
    st.subheader("Results")
    st.write("**BLIP (Base) Caption:**", caption_blip)
    st.write("**ViT-GPT2 Caption:**", caption_vit)
    st.write("**BLIP (Large) Caption:**", caption_blip_large)
    st.write("**Social Media Captions:**")
    for idx, cap in enumerate(social_captions):
        st.write(f"{idx+1}. {cap}")
    
    # Also print the final results array as a whole (if desired)
    st.write("**Final Results:**")
    st.write(results)
